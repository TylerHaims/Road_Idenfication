{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.utils._pytree' has no attribute 'register_pytree_node'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SegformerForSemanticSegmentation\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SegformerImageProcessor\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/DISenv/lib/python3.10/site-packages/transformers/__init__.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[1;32m     29\u001b[0m     _LazyModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     logging,\n\u001b[1;32m     49\u001b[0m )\n\u001b[1;32m     52\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/DISenv/lib/python3.10/site-packages/transformers/dependency_versions_check.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[1;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     38\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/DISenv/lib/python3.10/site-packages/transformers/utils/__init__.py:33\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     26\u001b[0m     add_code_sample_docstrings,\n\u001b[1;32m     27\u001b[0m     add_end_docstrings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     replace_return_docstrings,\n\u001b[1;32m     32\u001b[0m )\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     34\u001b[0m     ContextManagers,\n\u001b[1;32m     35\u001b[0m     ExplicitEnum,\n\u001b[1;32m     36\u001b[0m     ModelOutput,\n\u001b[1;32m     37\u001b[0m     PaddingStrategy,\n\u001b[1;32m     38\u001b[0m     TensorType,\n\u001b[1;32m     39\u001b[0m     add_model_info_to_auto_map,\n\u001b[1;32m     40\u001b[0m     cached_property,\n\u001b[1;32m     41\u001b[0m     can_return_loss,\n\u001b[1;32m     42\u001b[0m     expand_dims,\n\u001b[1;32m     43\u001b[0m     find_labels,\n\u001b[1;32m     44\u001b[0m     flatten_dict,\n\u001b[1;32m     45\u001b[0m     infer_framework,\n\u001b[1;32m     46\u001b[0m     is_jax_tensor,\n\u001b[1;32m     47\u001b[0m     is_numpy_array,\n\u001b[1;32m     48\u001b[0m     is_tensor,\n\u001b[1;32m     49\u001b[0m     is_tf_symbolic_tensor,\n\u001b[1;32m     50\u001b[0m     is_tf_tensor,\n\u001b[1;32m     51\u001b[0m     is_torch_device,\n\u001b[1;32m     52\u001b[0m     is_torch_dtype,\n\u001b[1;32m     53\u001b[0m     is_torch_tensor,\n\u001b[1;32m     54\u001b[0m     reshape,\n\u001b[1;32m     55\u001b[0m     squeeze,\n\u001b[1;32m     56\u001b[0m     strtobool,\n\u001b[1;32m     57\u001b[0m     tensor_size,\n\u001b[1;32m     58\u001b[0m     to_numpy,\n\u001b[1;32m     59\u001b[0m     to_py_obj,\n\u001b[1;32m     60\u001b[0m     transpose,\n\u001b[1;32m     61\u001b[0m     working_or_temp_dir,\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     64\u001b[0m     CLOUDFRONT_DISTRIB_PREFIX,\n\u001b[1;32m     65\u001b[0m     HF_MODULES_CACHE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m     try_to_load_from_cache,\n\u001b[1;32m     92\u001b[0m )\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     94\u001b[0m     ACCELERATE_MIN_VERSION,\n\u001b[1;32m     95\u001b[0m     ENV_VARS_TRUE_AND_AUTO_VALUES,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    204\u001b[0m     torch_only_method,\n\u001b[1;32m    205\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/DISenv/lib/python3.10/site-packages/transformers/utils/generic.py:478\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output_type(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(context, values)))\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(get_torch_version()) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.2\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 478\u001b[0m     \u001b[43m_torch_pytree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_pytree_node\u001b[49m(\n\u001b[1;32m    479\u001b[0m         ModelOutput,\n\u001b[1;32m    480\u001b[0m         _model_output_flatten,\n\u001b[1;32m    481\u001b[0m         partial(_model_output_unflatten, output_type\u001b[38;5;241m=\u001b[39mModelOutput),\n\u001b[1;32m    482\u001b[0m         serialized_type_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mModelOutput\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mModelOutput\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    483\u001b[0m     )\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    485\u001b[0m     _torch_pytree\u001b[38;5;241m.\u001b[39m_register_pytree_node(\n\u001b[1;32m    486\u001b[0m         ModelOutput,\n\u001b[1;32m    487\u001b[0m         _model_output_flatten,\n\u001b[1;32m    488\u001b[0m         partial(_model_output_unflatten, output_type\u001b[38;5;241m=\u001b[39mModelOutput),\n\u001b[1;32m    489\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.utils._pytree' has no attribute 'register_pytree_node'"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "from transformers import SegformerImageProcessor\n",
    "import json\n",
    "from huggingface_hub import hf_hub_download\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "import evaluate\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "from torch.utils.data import TensorDataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/DISenv/lib/python3.10/site-packages (4.39.3)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/DISenv/lib/python3.10/site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/anaconda3/envs/DISenv/lib/python3.10/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/DISenv/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/DISenv/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/DISenv/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/DISenv/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/DISenv/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/anaconda3/envs/DISenv/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/envs/DISenv/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/DISenv/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/DISenv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/DISenv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/DISenv/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/DISenv/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/DISenv/lib/python3.10/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/DISenv/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///Users/tylerhaims/Documents/GitHub/Road_Idenfication\n",
      "\u001b[31mERROR: file:///Users/tylerhaims/Documents/GitHub/Road_Idenfication does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "print('hi')\n",
    "\n",
    "# customizable id's\n",
    "road_labels = {\n",
    "  \"0\": \"not_road\",\n",
    "  \"1\": \"road\",\n",
    "}\n",
    "road_label2id = {v: k for k, v in road_labels.items()}\n",
    "\n",
    "\n",
    "# load id2label mapping from a JSON on the hub\n",
    "# think we might need to adjust the num_labels and the id2label situation to fit out specific case\n",
    "repo_id = \"huggingface/label-files\"\n",
    "filename = \"ade20k-id2label.json\"\n",
    "id2label = json.load(open(hf_hub_download(repo_id=repo_id, filename=filename, repo_type=\"dataset\"), \"r\"))\n",
    "id2label = {int(k): v for k, v in id2label.items()}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "print('hi2')\n",
    "# define model\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b0\",\n",
    "                                                         num_labels=2,\n",
    "                                                         id2label=road_labels,\n",
    "                                                         label2id=road_label2id,\n",
    ")\n",
    "\n",
    "print('hi3')\n",
    "# define the image processor\n",
    "image_processor = SegformerImageProcessor(do_reduce_labels=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# just kinda testing this out and figuring out if it works\n",
    "image = Image.open(\"/Users/rorybeals/Downloads/Road Identification Data/test/206_sat.jpg\")\n",
    "png_image = Image.open(\"/Users/rorybeals/Downloads/Road Identification Data/train/562_mask.png\")\n",
    "model.to(device)\n",
    "pixel_vals = image_processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "# forward pass\n",
    "with torch.no_grad():\n",
    "  outputs = model(pixel_values=pixel_vals)\n",
    "\n",
    "# took this from online to help with visualizing the segmentation\n",
    "def ade_palette():\n",
    "    \"\"\"ADE20K palette that maps each class to RGB values.\"\"\"\n",
    "    return [[120, 120, 120], [180, 120, 120], [6, 230, 230], [80, 50, 50],\n",
    "            [4, 200, 3], [120, 120, 80], [140, 140, 140], [204, 5, 255],\n",
    "            [230, 230, 230], [4, 250, 7], [224, 5, 255], [235, 255, 7],\n",
    "            [150, 5, 61], [120, 120, 70], [8, 255, 51], [255, 6, 82],\n",
    "            [143, 255, 140], [204, 255, 4], [255, 51, 7], [204, 70, 3],\n",
    "            [0, 102, 200], [61, 230, 250], [255, 6, 51], [11, 102, 255],\n",
    "            [255, 7, 71], [255, 9, 224], [9, 7, 230], [220, 220, 220],\n",
    "            [255, 9, 92], [112, 9, 255], [8, 255, 214], [7, 255, 224],\n",
    "            [255, 184, 6], [10, 255, 71], [255, 41, 10], [7, 255, 255],\n",
    "            [224, 255, 8], [102, 8, 255], [255, 61, 6], [255, 194, 7],\n",
    "            [255, 122, 8], [0, 255, 20], [255, 8, 41], [255, 5, 153],\n",
    "            [6, 51, 255], [235, 12, 255], [160, 150, 20], [0, 163, 255],\n",
    "            [140, 140, 140], [250, 10, 15], [20, 255, 0], [31, 255, 0],\n",
    "            [255, 31, 0], [255, 224, 0], [153, 255, 0], [0, 0, 255],\n",
    "            [255, 71, 0], [0, 235, 255], [0, 173, 255], [31, 0, 255],\n",
    "            [11, 200, 200], [255, 82, 0], [0, 255, 245], [0, 61, 255],\n",
    "            [0, 255, 112], [0, 255, 133], [255, 0, 0], [255, 163, 0],\n",
    "            [255, 102, 0], [194, 255, 0], [0, 143, 255], [51, 255, 0],\n",
    "            [0, 82, 255], [0, 255, 41], [0, 255, 173], [10, 0, 255],\n",
    "            [173, 255, 0], [0, 255, 153], [255, 92, 0], [255, 0, 255],\n",
    "            [255, 0, 245], [255, 0, 102], [255, 173, 0], [255, 0, 20],\n",
    "            [255, 184, 184], [0, 31, 255], [0, 255, 61], [0, 71, 255],\n",
    "            [255, 0, 204], [0, 255, 194], [0, 255, 82], [0, 10, 255],\n",
    "            [0, 112, 255], [51, 0, 255], [0, 194, 255], [0, 122, 255],\n",
    "            [0, 255, 163], [255, 153, 0], [0, 255, 10], [255, 112, 0],\n",
    "            [143, 255, 0], [82, 0, 255], [163, 255, 0], [255, 235, 0],\n",
    "            [8, 184, 170], [133, 0, 255], [0, 255, 92], [184, 0, 255],\n",
    "            [255, 0, 31], [0, 184, 255], [0, 214, 255], [255, 0, 112],\n",
    "            [92, 255, 0], [0, 224, 255], [112, 224, 255], [70, 184, 160],\n",
    "            [163, 0, 255], [153, 0, 255], [71, 255, 0], [255, 0, 163],\n",
    "            [255, 204, 0], [255, 0, 143], [0, 255, 235], [133, 255, 0],\n",
    "            [255, 0, 235], [245, 0, 255], [255, 0, 122], [255, 245, 0],\n",
    "            [10, 190, 212], [214, 255, 0], [0, 204, 255], [20, 0, 255],\n",
    "            [255, 255, 0], [0, 153, 255], [0, 41, 255], [0, 255, 204],\n",
    "            [41, 0, 255], [41, 255, 0], [173, 0, 255], [0, 245, 255],\n",
    "            [71, 0, 255], [122, 0, 255], [0, 255, 184], [0, 92, 255],\n",
    "            [184, 255, 0], [0, 133, 255], [255, 214, 0], [25, 194, 194],\n",
    "            [102, 255, 0], [92, 0, 255]]\n",
    "     \n",
    "predicted_segmentation_map = image_processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n",
    "predicted_segmentation_map = predicted_segmentation_map.cpu().numpy()\n",
    "print(predicted_segmentation_map)\n",
    "\n",
    "color_seg = np.zeros((predicted_segmentation_map.shape[0],\n",
    "                      predicted_segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
    "\n",
    "palette = np.array(ade_palette())\n",
    "for label, color in enumerate(palette):\n",
    "    color_seg[predicted_segmentation_map == label, :] = color\n",
    "# Convert to BGR\n",
    "color_seg = color_seg[..., ::-1]\n",
    "# cv2.imwrite(\"image_out.jpg\", color_seg)\n",
    "\n",
    "# Show image + mask\n",
    "img = np.array(image) * 0.5 + color_seg * 0.5\n",
    "img = img.astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "print('ahhhhh')\n",
    "# this function is supposed to process the png files and encode them with proper values that we can use to retrain the model a bit\n",
    "def process_png(filename):\n",
    "    image = Image.open(filename)\n",
    "    binary_mask = np.array(image.convert(\"L\"))\n",
    "    binary_mask = np.where(binary_mask > 0, 1, 0)\n",
    "    encoded_mask = np.zeros_like(binary_mask, dtype=np.int64)\n",
    "    for label, class_name in road_labels.items():\n",
    "        encoded_mask[binary_mask == label] = label\n",
    "    return torch.tensor(encoded_mask)\n",
    "\n",
    "# gonna see if we can train this thangggg\n",
    "# get all of the filenames separated\n",
    "path = '/Users/rorybeals/Downloads/Road Identification Data/train/'\n",
    "jpg_files = []\n",
    "png_files = []\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith('.jpg'):\n",
    "        jpg_files.append(file)\n",
    "    else: png_files.append(file)\n",
    "\n",
    "# define our training data and labels\n",
    "training_images = []\n",
    "training_labels = []\n",
    "\n",
    "data_dict = defaultdict(dict)\n",
    "for file in os.listdir(path):\n",
    "    num, type_ = file.split('_')\n",
    "    data_dict[num][type_] = file\n",
    "\n",
    "# set up the training_images and training_labels lists of filenames\n",
    "for file in jpg_files:\n",
    "    file_num = file.split('_')[0]\n",
    "    filename = path + '/' + file\n",
    "    training_images.append(filename)\n",
    "    png_image = data_dict[file_num]['mask.png']\n",
    "    png_filename = path + '/'+  png_image\n",
    "    training_labels.append(png_filename)\n",
    "    if (len(training_images) == 20):\n",
    "        break\n",
    "\n",
    "processed_images = []\n",
    "processed_labels = []\n",
    "\n",
    "for image_filename, label_filename in zip(training_images, training_labels):\n",
    "    sat_image = Image.open(image_filename)\n",
    "    pixel_vals = image_processor(sat_image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    processed_images.append(pixel_vals)\n",
    "    labels = process_png(png_filename)\n",
    "    processed_labels.append(labels)\n",
    "\n",
    "print('hi4')\n",
    "\n",
    "# Convert lists of tensors to PyTorch tensors\n",
    "processed_images_tensor = torch.stack(processed_images)\n",
    "processed_labels_tensor = torch.stack(processed_labels)\n",
    "\n",
    "dataset = TensorDataset(processed_images_tensor, processed_labels_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# set up some batches\n",
    "training_images_tensor = tf.convert_to_tensor(training_images)\n",
    "training_labels_tensor = tf.convert_to_tensor(training_labels)\n",
    "\n",
    "print('hi5')\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "image_processor = SegformerImageProcessor(do_reduce_labels=True)\n",
    "model.to(device)\n",
    "# tune the model\n",
    "print('hi6')\n",
    "model.train()\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    print(\"Epoch:\", epoch)\n",
    "    for batch in dataloader:\n",
    "        print('batch running')\n",
    "        # get the inputs;\n",
    "        pixel_values, labels = batch\n",
    "        pixel_values = pixel_values.to(device)\n",
    "        pixel_values = pixel_values[:, 0, :, :, :]\n",
    "        labels = labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        # optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('gabagoo4')\n",
    "        # evaluate\n",
    "        with torch.no_grad():\n",
    "          upsampled_logits = nn.functional.interpolate(logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "          predicted = upsampled_logits.argmax(dim=1)\n",
    "\n",
    "          # note that the metric expects predictions + labels as numpy arrays\n",
    "          metric.add_batch(predictions=predicted.detach().cpu().numpy(), references=labels.detach().cpu().numpy())\n",
    "          metrics = metric._compute(\n",
    "                  predictions=predicted.cpu(),\n",
    "                  references=labels.cpu(),\n",
    "                  num_labels=len(id2label),\n",
    "                  ignore_index=255,\n",
    "                  reduce_labels=False, # we've already reduced the labels ourselves\n",
    "              )\n",
    "        print('batch over')\n",
    "\n",
    "    print(\"Loss:\", loss.item())\n",
    "    print(\"Mean_iou:\", metrics[\"mean_iou\"])\n",
    "    print(\"Mean accuracy:\", metrics[\"mean_accuracy\"])\n",
    "\n",
    "# just kinda testing this out and figuring out if it works\n",
    "image = Image.open(\"/Users/rorybeals/Downloads/Road Identification Data/test/206_sat.jpg\")\n",
    "png_image = Image.open(\"/Users/rorybeals/Downloads/Road Identification Data/train/562_mask.png\")\n",
    "pixel_vals = image_processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "# forward pass\n",
    "with torch.no_grad():\n",
    "  outputs = model(pixel_values=pixel_vals)\n",
    "\n",
    "     \n",
    "predicted_segmentation_map = image_processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n",
    "predicted_segmentation_map = predicted_segmentation_map.cpu().numpy()\n",
    "print(predicted_segmentation_map)\n",
    "\n",
    "color_seg = np.zeros((predicted_segmentation_map.shape[0],\n",
    "                      predicted_segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
    "\n",
    "palette = np.array(ade_palette())\n",
    "for label, color in enumerate(palette):\n",
    "    color_seg[predicted_segmentation_map == label, :] = color\n",
    "# Convert to BGR\n",
    "color_seg = color_seg[..., ::-1]\n",
    "\n",
    "# Show image + mask\n",
    "img = np.array(image) * 0.5 + color_seg * 0.5\n",
    "img = img.astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchgeo\n",
      "  Downloading torchgeo-0.5.2-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting einops>=0.3 (from torchgeo)\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting fiona>=1.8.19 (from torchgeo)\n",
      "  Downloading fiona-1.9.6.tar.gz (411 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.0/411.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[3 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m <string>:86: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "  \u001b[31m   \u001b[0m WARNING:root:Failed to get options via gdal-config: [Errno 2] No such file or directory: 'gdal-config'\n",
      "  \u001b[31m   \u001b[0m CRITICAL:root:A GDAL API version must be specified. Provide a path to gdal-config using a GDAL_CONFIG environment variable or use a GDAL_VERSION environment variable.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchgeo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DISenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
